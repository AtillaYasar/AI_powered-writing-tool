{
    "F1": "!!!bla\nfear\n=====\n!!!bla\ntest",
    "F2": "!!!barack obama\n!!!wiki\nBarack Hussein Obama II (/b\u0259\u02c8r\u0251\u02d0k hu\u02d0\u02c8se\u026an o\u028a\u02c8b\u0251\u02d0m\u0259/ (listen) b\u0259-RAHK hoo-SAYN oh-BAH-m\u0259;[1] born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president of the United States.[2] Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004, and worked as a civil rights lawyer before holding public office.\n\nObama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received keynote address at the July Democratic National Convention, and his landslide November election to the Senate. In 2008, after a close primary campaign against Hillary Clinton, he was nominated by the Democratic Party for president and chose Joe Biden as his running mate. Obama was elected over Republican nominee John McCain in the presidential election and was inaugurated on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate, a decision that drew a mixture of praise and criticism.\n\nObama's first-term actions addressed the global financial crisis and included a major stimulus package, a partial extension of George W. Bush's tax cuts, legislation to reform health care, a major financial regulation reform bill, and the end of a major US military presence in Iraq. Obama also appointed Supreme Court justices Sonia Sotomayor and Elena Kagan, the former being the first Hispanic American on the Supreme Court. He ordered the counterterrorism raid which killed Osama bin Laden and downplayed Bush's counterinsurgency model, expanding air strikes and making extensive use of special forces while encouraging greater reliance on host-government militaries.\n\nAfter winning re-election by defeating Republican opponent Mitt Romney, Obama was sworn in for a second term on January 20, 2013. In his second term, Obama took steps to combat climate change, signing a major international climate agreement and an executive order to limit carbon emissions. Obama also presided over the implementation of the Affordable Care Act and other legislation passed in his first term, and he negotiated a nuclear agreement with Iran and normalized relations with Cuba. The number of American soldiers in Afghanistan fell dramatically during Obama's second term, though U.S. soldiers remained in Afghanistan throughout Obama's presidency.\n\nDuring Obama's terms as president, the United States' reputation abroad and the American economy improved significantly, although the country experienced high levels of partisan divide. Obama left office on January 20, 2017, and continues to reside in Washington, D.C. His presidential library in Chicago began construction in 2021. Since leaving office, Obama has remained active in Democratic politics, including campaigning for candidates in various American elections. Outside of politics, Obama has published three bestselling books: Dreams from My Father (1995), The Audacity of Hope (2006) and A Promised Land (2020). Rankings by scholars and historians, in which he has been featured since 2010, place him in the middle to upper tier of American presidents.[3][4][5]\n=====\n!!!obama\n!!!early life and career\n!!!wiki\nEarly life and career\nMain article: Early life and career of Barack Obama\nPhoto of a young Obama sitting on grass with his grandfather, mother, and half-sister.\nObama (right) with grandfather Stanley Armour Dunham, mother Ann Dunham, and sister Maya Soetoro, mid-1970s in Honolulu\nObama was born on August 4, 1961,[6] at Kapiolani Medical Center for Women and Children in Honolulu, Hawaii.[7][8][9] He is the only president born outside the contiguous 48 states.[10] He was born to an American mother and a Kenyan father. His mother, Ann Dunham (1942\u20131995), was born in Wichita, Kansas and was of English, Welsh, German, Swiss, and Irish descent. In 2007 it was discovered her great-great-grandfather Falmouth Kearney emigrated from the village of Moneygall, Ireland to the US in 1850.[11] In July 2012, Ancestry.com found a strong likelihood that Dunham was descended from John Punch, an enslaved African man who lived in the Colony of Virginia during the seventeenth century.[12][13] Obama's father, Barack Obama Sr. (1934\u20131982),[14][15] was a married[16][17][18] Luo Kenyan from Nyang'oma Kogelo.[16][19] Obama's parents met in 1960 in a Russian language class at the University of Hawaii at Manoa, where his father was a foreign student on a scholarship.[20][21] The couple married in Wailuku, Hawaii, on February 2, 1961, six months before Obama was born.[22][23]\n\nIn late August 1961, a few weeks after he was born, Barack and his mother moved to the University of Washington in Seattle, where they lived for a year. During that time, Barack's father completed his undergraduate degree in economics in Hawaii, graduating in June 1962. He left to attend graduate school on a scholarship at Harvard University, where he earned an M.A. in economics. Obama's parents divorced in March 1964.[24] Obama Sr. returned to Kenya in 1964, where he married for a third time and worked for the Kenyan government as the Senior Economic Analyst in the Ministry of Finance.[25] He visited his son in Hawaii only once, at Christmas 1971,[26] before he was killed in an automobile accident in 1982, when Obama was 21 years old.[27] Recalling his early childhood, Obama said: \"That my father looked nothing like the people around me\u2014that he was black as pitch, my mother white as milk\u2014barely registered in my mind.\"[21] He described his struggles as a young adult to reconcile social perceptions of his multiracial heritage.[28]\n=====\n!!!obama\n!!!wiki\nIn 1963, Dunham met Lolo Soetoro at the University of Hawaii; he was an Indonesian East\u2013West Center graduate student in geography. The couple married on Molokai on March 15, 1965.[29] After two one-year extensions of his J-1 visa, Lolo returned to Indonesia in 1966. His wife and stepson followed sixteen months later in 1967. The family initially lived in the Menteng Dalam neighborhood in the Tebet district of South Jakarta. From 1970, they lived in a wealthier neighborhood in the Menteng district of Central Jakarta.[30]\n=====\nsup bruh\n=====\nembed me uwu\n=====\nlets go solve alignment yo",
    "F3": "!!!death with dignity\n!!!eliezer yudkowsky\n!!!lesswrong\ntl;dr:  It's obvious at this point that humanity isn't going to solve the alignment problem, or even try very hard, or even go out with much of a fight.  Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity.\n\nWell, let's be frank here.  MIRI didn't solve AGI alignment and at least knows that it didn't.  Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world.  Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.\n\nManagement will then ask what they're supposed to do about that.\n\nWhoever detected the warning sign will say that there isn't anything known they can do about that.  Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that.  Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways.  Pretty soon that troublesome error signal will vanish.\n\nWhen Earth's prospects are that far underwater in the basement of the logistic success curve, it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.\n\nThat's why I would suggest reframing the problem - especially on an emotional level - to helping humanity die with dignity, or rather, since even this goal is realistically unattainable at this point, die with slightly more dignity than would otherwise be counterfactually obtained.\n\nConsider the world if Chris Olah had never existed.  It's then much more likely that nobody will even try and fail to adapt Olah's methodologies to try and read complicated facts about internal intentions and future plans, out of whatever enormous inscrutable tensors are being integrated a million times per second, inside of whatever recently designed system finished training 48 hours ago, in a vast GPU farm that's already helpfully connected to the Internet.\n\nIt is more dignified for humanity - a better look on our tombstone - if we die after the management of the AGI project was heroically warned of the dangers but came up with totally reasonable reasons to go ahead anyways.\n\nOr, failing that, if people made a heroic effort to do something that could maybe possibly have worked to generate a warning like that but couldn't actually in real life because the latest tensors were in a slightly different format and there was no time to readapt the methodology.  Compared to the much less dignified-looking situation if there's no warning and nobody even tried to figure out how to generate one.\n\nOr take MIRI.  Are we sad that it looks like this Earth is going to fail?  Yes.  Are we sad that we tried to do anything about that?  No, because it would be so much sadder, when it all ended, to face our ends wondering if maybe solving alignment would have just been as easy as buckling down and making a serious effort on it - not knowing if that would've just worked, if we'd only tried, because nobody had ever even tried at all.  It wasn't subjectively overdetermined that the (real) problems would be too hard for us, before we made the only attempt at solving them that would ever be made.  Somebody needed to try at all, in case that was all it took.\n\nIt's sad that our Earth couldn't be one of the more dignified planets that makes a real effort, correctly pinpointing the actual real difficult problems and then allocating thousands of the sort of brilliant kids that our Earth steers into wasting their lives on theoretical physics.  But better MIRI's effort than nothing.  What were we supposed to do instead, pick easy irrelevant fake problems that we could make an illusion of progress on, and have nobody out of the human species even try to solve the hard scary real problems, until everybody just fell over dead?\n\nThis way, at least, some people are walking around knowing why it is that if you train with an outer loss function that enforces the appearance of friendliness, you will not get an AI internally motivated to be friendly in a way that persists after its capabilities start to generalize far out of the training distribution...\n\nTo be clear, nobody's going to listen to those people, in the end.  There will be more comforting voices that sound less politically incongruent with whatever agenda needs to be pushed forward that week.  Or even if that ends up not so, this isn't primarily a social-political problem, of just getting people to listen.  Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.\n\nBut compared to being part of a species that walks forward completely oblivious into the whirling propeller blades, with nobody having seen it at all or made any effort to stop it, it is dying with a little more dignity, if anyone knew at all.  You can feel a little incrementally prouder to have died as part of a species like that, if maybe not proud in absolute terms.\n\nIf there is a stronger warning, because we did more transparency research?  If there's deeper understanding of the real dangers and those come closer to beating out comfortable nonrealities, such that DeepMind and Anthropic really actually back off from destroying the world and let Facebook AI Research do it instead?  If they try some hopeless alignment scheme whose subjective success probability looks, to the last sane people, more like 0.1% than 0?  Then we have died with even more dignity!  It may not get our survival probabilities much above 0%, but it would be so much more dignified than the present course looks to be!\n\nNow of course the real subtext here, is that if you can otherwise set up the world so that it looks like you'll die with enough dignity - die of the social and technical problems that are really unavoidable, after making a huge effort at coordination and technical solutions and failing, rather than storming directly into the whirling helicopter blades as is the present unwritten plan -\n\n- heck, if there was even a plan at all -\n\n- then maybe possibly, if we're wrong about something fundamental, somehow, somewhere -\n\n- in a way that makes things easier rather than harder, because obviously we're going to be wrong about all sorts of things, it's a whole new world inside of AGI -\n\n- although, when you're fundamentally wrong about rocketry, this does not usually mean your rocket prototype goes exactly where you wanted on the first try while consuming half as much fuel as expected; it means the rocket explodes earlier yet, and not in a way you saw coming, being as wrong as you were -\n\n- but if we get some miracle of unexpected hope, in those unpredicted inevitable places where our model is wrong -\n\n- then our ability to take advantage of that one last hope, will greatly depend on how much dignity we were set to die with, before then.\n\nIf we can get on course to die with enough dignity, maybe we won't die at all...?\n\nIn principle, yes.  Let's be very clear, though:  Realistically speaking, that is not how real life works.\n\nIt's possible for a model error to make your life easier.  But you do not get more surprises that make your life easy, than surprises that make your life even more difficult.  And people do not suddenly become more reasonable, and make vastly more careful and precise decisions, as soon as they're scared.  No, not even if it seems to you like their current awful decisions are weird and not-in-the-should-universe, and surely some sharp shock will cause them to snap out of that weird state into a normal state and start outputting the decisions you think they should make.\n\nSo don't get your heart set on that \"not die at all\" business.  Don't invest all your emotion in a reward you probably won't get.  Focus on dying with dignity - that is something you can actually obtain, even in this situation.  After all, if you help humanity die with even one more dignity point, you yourself die with one hundred dignity points!  Even if your species dies an incredibly undignified death, for you to have helped humanity go down with even slightly more of a real fight, is to die an extremely dignified death.\n\n\"Wait, dignity points?\" you ask.  \"What are those?  In what units are they measured, exactly?\"\n\nAnd to this I reply:  Obviously, the measuring units of dignity are over humanity's log odds of survival - the graph on which the logistic success curve is a straight line.  A project that doubles humanity's chance of survival from 0% to 0% is helping humanity die with one additional information-theoretic bit of dignity.\n\nBut if enough people can contribute enough bits of dignity like that, wouldn't that mean we didn't die at all?  Yes, but again, don't get your hopes up.  Don't focus your emotions on a goal you're probably not going to obtain.  Realistically, we find a handful of projects that contribute a few more bits of counterfactual dignity; get a bunch more not-specifically-expected bad news that makes the first-order object-level situation look even worse (where to second order, of course, the good Bayesians already knew that was how it would go); and then we all die.\n\nWith a technical definition in hand of what exactly constitutes dignity, we may now consider some specific questions about what does and doesn't constitute dying with dignity.\n\nQ1:  Does 'dying with dignity' in this context mean accepting the certainty of your death, and not childishly regretting that or trying to fight a hopeless battle?\n\nDon't be ridiculous.  How would that increase the log odds of Earth's survival?\n\nMy utility function isn't up for grabs, either.  If I regret my planet's death then I regret it, and it's beneath my dignity to pretend otherwise.\n\nThat said, I fought hardest while it looked like we were in the more sloped region of the logistic success curve, when our survival probability seemed more around the 50% range; I borrowed against my future to do that, and burned myself out to some degree.  That was a deliberate choice, which I don't regret now; it was worth trying, I would not have wanted to die having not tried, I would not have wanted Earth to die without anyone having tried.  But yeah, I am taking some time partways off, and trying a little less hard, now.  I've earned a lot of dignity already; and if the world is ending anyways and I can't stop it, I can afford to be a little kind to myself about that.\n\nWhen I tried hard and burned myself out some, it was with the understanding, within myself, that I would not keep trying to do that forever.  We cannot fight at maximum all the time, and some times are more important than others.  (Namely, when the logistic success curve seems relatively more sloped; those times are relatively more important.)\n\nAll that said:  If you fight marginally longer, you die with marginally more dignity.  Just don't undignifiedly delude yourself about the probable outcome.\n\nQ2:  I have a clever scheme for saving the world!  I should act as if I believe it will work and save everyone, right, even if there's arguments that it's almost certainly misguided and doomed?  Because if those arguments are correct and my scheme can't work, we're all dead anyways, right?\n\nA:  No!  That's not dying with dignity!  That's stepping sideways out of a mentally uncomfortable world and finding an escape route from unpleasant thoughts!  If you condition your probability models on a false fact, something that isn't true on the mainline, it means you've mentally stepped out of reality and are now living somewhere else instead.\n\nThere are more elaborate arguments against the rationality of this strategy, but consider this quick heuristic for arriving at the correct answer:  That's not a dignified way to die.  Death with dignity means going on mentally living in the world you think is reality, even if it's a sad reality, until the end; not abandoning your arts of seeking truth; dying with your commitment to reason intact.\n\nYou should try to make things better in the real world, where your efforts aren't enough and you're going to die anyways; not inside a fake world you can save more easily.\n\nQ2:  But what's wrong with the argument from expected utility, saying that all of humanity's expected utility lies within possible worlds where my scheme turns out to be feasible after all?\n\nA:  Most fundamentally?  That's not what the surviving worlds look like.  The surviving worlds look like people who lived inside their awful reality and tried to shape up their impossible chances; until somehow, somewhere, a miracle appeared - the model broke in a positive direction, for once, as does not usually occur when you are trying to do something very difficult and hard to understand, but might still be so - and they were positioned with the resources and the sanity to take advantage of that positive miracle, because they went on living inside uncomfortable reality.  Positive model violations do ever happen, but it's much less likely that somebody's specific desired miracle that \"we're all dead anyways if not...\" will happen; these people have just walked out of the reality where any actual positive miracles might occur.\n\nAlso and in practice?  People don't just pick one comfortable improbability to condition on.  They go on encountering unpleasant facts true on the mainline, and each time saying, \"Well, if that's true, I'm doomed, so I may as well assume it's not true,\" and they say more and more things like this.  If you do this it very rapidly drives down the probability mass of the 'possible' world you're mentally inhabiting.  Pretty soon you're living in a place that's nowhere near reality.  If there were an expected utility argument for risking everything on an improbable assumption, you'd get to make exactly one of them, ever.  People using this kind of thinking usually aren't even keeping track of when they say it, let alone counting the occasions.\n\nAlso also, in practice?  In domains like this one, things that seem to first-order like they \"might\" work... have essentially no chance of working in real life, to second-order after taking into account downward adjustments against optimism.  AGI is a scientifically unprecedented experiment and a domain with lots of optimization pressures some of which work against you and unforeseeable intelligently selected execution pathways and with a small target to hit and all sorts of extreme forces that break things and that you couldn't fully test before facing them.  AGI alignment seems like it's blatantly going to be an enormously Murphy-cursed domain, like rocket prototyping or computer security but worse.\n\nIn a domain like, if you have a clever scheme for winning anyways that, to first-order theoretical theory, totally definitely seems like it should work, even to Eliezer Yudkowsky rather than somebody who just goes around saying that casually, then maybe there's like a 50% chance of it working in practical real life after all the unexpected disasters and things turning out to be harder than expected.\n\nIf to first-order it seems to you like something in a complicated unknown untested domain has a 40% chance of working, it has a 0% chance of working in real life.\n\nAlso also also in practice?  Harebrained schemes of this kind are usually actively harmful.  Because they're invented by the sort of people who'll come up with an unworkable scheme, and then try to get rid of counterarguments with some sort of dismissal like \"Well if not then we're all doomed anyways.\"\n\nIf nothing else, this kind of harebrained desperation drains off resources from those reality-abiding efforts that might try to do something on the subjectively apparent doomed mainline, and so position themselves better to take advantage of unexpected hope, which is what the surviving possible worlds mostly look like.\n\nThe surviving worlds don't look like somebody came up with a harebrained scheme, dismissed all the obvious reasons it wouldn't work with \"But we have to bet on it working,\" and then it worked.\n\nThat's the elaborate argument about what's rational in terms of expected utility, once reasonable second-order commonsense adjustments are taken into account.  Note, however, that if you have grasped the intended emotional connotations of \"die with dignity\", it's a heuristic that yields the same answer much faster.  It's not dignified to pretend we're less doomed than we are, or step out of reality to live somewhere else.\n\nQ3:  Should I scream and run around and go through the streets wailing of doom?\n\nA:  No, that's not very dignified.  Have a private breakdown in your bedroom, or a breakdown with a trusted friend, if you must.\n\nQ3:  Why is that bad from a coldly calculating expected utility perspective, though?\n\nA:  Because it associates belief in reality with people who act like idiots and can't control their emotions, which worsens our strategic position in possible worlds where we get an unexpected hope.\n\nQ4:  Should I lie and pretend everything is fine, then?  Keep everyone's spirits up, so they go out with a smile, unknowing?\n\nA:  That also does not seem to me to be dignified.  If we're all going to die anyways, I may as well speak plainly before then.  If into the dark we must go, let's go there speaking the truth, to others and to ourselves, until the end.\n\nQ4:  Okay, but from a coldly calculating expected utility perspective, why isn't it good to lie to keep everyone calm?  That way, if there's an unexpected hope, everybody else will be calm and oblivious and not interfering with us out of panic, and my faction will have lots of resources that they got from lying to their supporters about how much hope there was!  Didn't you just say that people screaming and running around while the world was ending would be unhelpful?\n\nA:  You should never try to reason using expected utilities again.  It is an art not meant for you.  Stick to intuitive feelings henceforth.\n\nThere are, I think, people whose minds readily look for and find even the slightly-less-than-totally-obvious considerations of expected utility, what some might call \"second-order\" considerations.  Ask them to rob a bank and give the money to the poor, and they'll think spontaneously and unprompted about insurance costs of banking and the chance of getting caught and reputational repercussions and low-trust societies and what if everybody else did that when they thought it was a good cause; and all of these considerations will be obviously-to-them consequences under consequentialism.\n\nThese people are well-suited to being 'consequentialists' or 'utilitarians', because their mind naturally sees all the consequences and utilities, including those considerations that others might be tempted to call by names like \"second-order\" or \"categorical\" and so on.\n\nIf you ask them why consequentialism doesn't say to rob banks, they reply, \"Because that actually realistically in real life would not have good consequences.  Whatever it is you're about to tell me as a supposedly non-consequentialist reason why we all mustn't do that, seems to you like a strong argument, exactly because you recognize implicitly that people robbing banks would not actually lead to happy formerly-poor people and everybody living cheerfully ever after.\"\n\nOthers, if you suggest to them that they should rob a bank and give the money to the poor, will be able to see the helped poor as a \"consequence\" and a \"utility\", but they will not spontaneously and unprompted see all those other considerations in the formal form of \"consequences\" and \"utilities\".\n\nIf you just asked them informally whether it was a good or bad idea, they might ask \"What if everyone did that?\" or \"Isn't it good that we can live in a society where people can store and transmit money?\" or \"How would it make effective altruism look, if people went around doing that in the name of effective altruism?\"  But if you ask them about consequences, they don't spontaneously, readily, intuitively classify all these other things as \"consequences\"; they think that their mind is being steered onto a kind of formal track, a defensible track, a track of stating only things that are very direct or blatant or obvious.  They think that the rule of consequentialism is, \"If you show me a good consequence, I have to do that thing.\"\n\nIf you present them with bad things that happen if people rob banks, they don't see those as also being 'consequences'.  They see them as arguments against consequentialism; since, after all consequentialism says to rob banks, which obviously leads to bad stuff, and so bad things would end up happening if people were consequentialists.  They do not do a double-take and say \"What?\"  That consequentialism leads people to do bad things with bad outcomes is just a reasonable conclusion, so far as they can tell.\n\nPeople like this should not be 'consequentialists' or 'utilitarians' as they understand those terms.  They should back off from this form of reasoning that their mind is not naturally well-suited for processing in a native format, and stick to intuitively informally asking themselves what's good or bad behavior, without any special focus on what they think are 'outcomes'.\n\nIf they try to be consequentialists, they'll end up as Hollywood villains describing some grand scheme that violates a lot of ethics and deontology but sure will end up having grandiose benefits, yup, even while everybody in the audience knows perfectly well that it won't work.  You can only safely be a consequentialist if you're genre-savvy about that class of arguments - if you're not the blind villain on screen, but the person in the audience watching who sees why that won't work.\n\nQ4:  I know EAs shouldn't rob banks, so this obviously isn't directed at me, right?\n\nA:  The people of whom I speak will look for and find the reasons not to do it, even if they're in a social environment that doesn't have strong established injunctions against bank-robbing specifically exactly.  They'll figure it out even if you present them with a new problem isomorphic to bank-robbing but with the details changed.\n\nWhich is basically what you just did, in my opinion.\n\nQ4:  But from the standpoint of cold-blooded calculation -\n\nA:  Calculations are not cold-blooded.  What blood we have in us, warm or cold, is something we can learn to see more clearly with the light of calculation.\n\nIf you think calculations are cold-blooded, that they only shed light on cold things or make them cold, then you shouldn't do them.  Stay by the warmth in a mental format where warmth goes on making sense to you.\n\nQ4:  Yes yes fine fine but what's the actual downside from an expected-utility standpoint?\n\nA:  If good people were liars, that would render the words of good people meaningless as information-theoretic signals, and destroy the ability for good people to coordinate with others or among themselves.\n\nIf the world can be saved, it will be saved by people who didn't lie to themselves, and went on living inside reality until some unexpected hope appeared there.\n\nIf those people went around lying to others and paternalistically deceiving them - well, mostly, I don't think they'll have really been the types to live inside reality themselves.  But even imagining the contrary, good luck suddenly unwinding all those deceptions and getting other people to live inside reality with you, to coordinate on whatever suddenly needs to be done when hope appears, after you drove them outside reality before that point.  Why should they believe anything you say?\n\nQ4:  But wouldn't it be more clever to -\n\nA:  Stop.  Just stop.  This is why I advised you to reframe your emotional stance as dying with dignity.\n\nMaybe there'd be an argument about whether or not to violate your ethics if the world was actually going to be saved at the end.  But why break your deontology if it's not even going to save the world?  Even if you have a price, should you be that cheap?\n\nQ4  But we could maybe save the world by lying to everyone about how much hope there was, to gain resources, until -\n\nA:  You're not getting it.  Why violate your deontology if it's not going to really actually save the world in real life, as opposed to a pretend theoretical thought experiment where your actions have only beneficial consequences and none of the obvious second-order detriments?\n\nIt's relatively safe to be around an Eliezer Yudkowsky while the world is ending, because he's not going to do anything extreme and unethical unless it would really actually save the world in real life, and there are no extreme unethical actions that would really actually save the world the way these things play out in real life, and he knows that.  He knows that the next stupid sacrifice-of-ethics proposed won't work to save the world either, actually in real life.  He is a 'pessimist' - that is, a realist, a Bayesian who doesn't update in a predictable direction, a genre-savvy person who knows that the viewer would say if there were a villain on screen making that argument for violating ethics.  He will not, like a Hollywood villain onscreen, be deluded into thinking that some clever-sounding deontology-violation is bound to work out great, when everybody in the audience watching knows perfectly well that it won't.\n\nMy ethics aren't for sale at the price point of failure.  So if it looks like everything is going to fail, I'm a relatively safe person to be around.\n\nI'm a genre-savvy person about this genre of arguments and a Bayesian who doesn't update in a predictable direction.  So if you ask, \"But Eliezer, what happens when the end of the world is approaching, and in desperation you cling to whatever harebrained scheme has Goodharted past your filters and presented you with a false shred of hope; what then will you do?\" - I answer, \"Die with dignity.\"  Where \"dignity\" in this case means knowing perfectly well that's what would happen to some less genre-savvy person; and my choosing to do something else which is not that.  But \"dignity\" yields the same correct answer and faster.\n\nQ5:  \"Relatively\" safe?\n\nA:  It'd be disingenuous to pretend that it wouldn't be even safer to hang around somebody who had no clue what was coming, didn't know any mental motions for taking a worldview seriously, thought it was somebody else's problem to ever do anything, and would just cheerfully party with you until the end.\n\nWithin the class of people who know the world is ending and consider it to be their job to do something about that, Eliezer Yudkowsky is a relatively safe person to be standing next to.  At least, before you both die anyways, as is the whole problem there.\n\nQ5:  Some of your self-proclaimed fans don't strike me as relatively safe people to be around, in that scenario?\n\nA:  I failed to teach them whatever it is I know.  Had I known then what I knew now, I would have warned them not to try.\n\nIf you insist on putting it into terms of fandom, though, feel free to notice that Eliezer Yudkowsky is much closer to being a typical liberaltarian science-fiction fan, as was his own culture that actually birthed him, than he is a typical member of any subculture that might have grown up later.  Liberaltarian science-fiction fans do not usually throw away all their ethics at the first sign of trouble.  They grew up reading books where those people were the villains.\n\nPlease don't take this as a promise from me to play nice, as you define niceness; the world is ending, and also people have varying definitions of what is nice.  But I presently mostly expect to end up playing nice, because there won't be any options worth playing otherwise.\n\nIt is a matter of some concern to me that all this seems to be an alien logic to some strange people who - this fact is still hard for me to grasp on an emotional level - don't spontaneously generate all of this reasoning internally, as soon as confronted with the prompt.  Alas.\n\nQ5:  Then isn't it unwise to speak plainly of these matters, when fools may be driven to desperation by them?  What if people believe you about the hopeless situation, but refuse to accept that conducting themselves with dignity is the appropriate response?\n\nA:  I feel like I've now tried to live my life that way for a while, by the dictum of not panicking people; and, like everything else I've tried, that hasn't particularly worked?  There are no plans left to avoid disrupting, now, with other people's hypothetical panic.\n\nI think we die with slightly more dignity - come closer to surviving, as we die - if we are allowed to talk about these matters plainly.  Even given that people may then do unhelpful things, after being driven mad by overhearing sane conversations.  I think we die with more dignity that way, than if we go down silent and frozen and never talking about our impending death for fear of being overheard by people less sane than ourselves.\n\nI think that in the last surviving possible worlds with any significant shred of subjective probability, people survived in part because they talked about it; even if that meant other people, the story's antagonists, might possibly hypothetically panic.\n\nBut still, one should present the story-antagonists with an easy line of retreat.  So -\n\nQ6:  Hey, this was posted on April 1st.  All of this is just an April Fool's joke, right?\n\nA:  Why, of course!  Or rather, it's a preview of what might be needful to say later, if matters really do get that desperate.  You don't want to drop that on people suddenly and with no warning.\n\nQ6:  Oh.  Really?  That would be such a relief!\n\nA:  Only you can decide whether to live in one mental world or the other.\n\nQ6:  Wait, now I'm confused.  How do I decide which mental world to live in?\n\nA:  By figuring out what is true, and by allowing no other considerations than that to enter; that's dignity.\n\nQ6:  But that doesn't directly answer the question of which world I'm supposed to mentally live in!  Can't somebody just tell me that?\n\nA:  Well, conditional on you wanting somebody to tell you that, I'd remind you that many EAs hold that it is very epistemically unvirtuous to just believe what one person tells you, and not weight their opinion and mix it with the weighted opinions of others?\n\nLots of very serious people will tell you that AGI is thirty years away, and that's plenty of time to turn things around, and nobody really knows anything about this subject matter anyways, and there's all kinds of plans for alignment that haven't been solidly refuted so far as they can tell.\n\nI expect the sort of people who are very moved by that argument, to be happier, more productive, and less disruptive, living mentally in that world.\n\nQ6:  Thanks for answering my question!  But aren't I supposed to assign some small probability to your worldview being correct?\n\nA:  Conditional on you being the sort of person who thinks you're obligated to do that and that's the reason you should do it, I'd frankly rather you didn't.  Or rather, seal up that small probability in a safe corner of your mind which only tells you to stay out of the way of those gloomy people, and not get in the way of any hopeless plans they seem to have.\n\nQ6:  Got it.  Thanks again!\n\nA:  You're welcome!  Goodbye and have fun!",
    "F4": "!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n!!!tldr\ntl;dr:  It's obvious at this point that humanity isn't going to solve the alignment problem, or even try very hard, or even go out with much of a fight.  Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nWell, let's be frank here.  MIRI didn't solve AGI alignment and at least knows that it didn't.  Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world.  Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nManagement will then ask what they're supposed to do about that.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nWhoever detected the warning sign will say that there isn't anything known they can do about that.  Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that.  Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways.  Pretty soon that troublesome error signal will vanish.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nWhen Earth's prospects are that far underwater in the basement of the logistic success curve, it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nThat's why I would suggest reframing the problem - especially on an emotional level - to helping humanity die with dignity, or rather, since even this goal is realistically unattainable at this point, die with slightly more dignity than would otherwise be counterfactually obtained.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nConsider the world if Chris Olah had never existed.  It's then much more likely that nobody will even try and fail to adapt Olah's methodologies to try and read complicated facts about internal intentions and future plans, out of whatever enormous inscrutable tensors are being integrated a million times per second, inside of whatever recently designed system finished training 48 hours ago, in a vast GPU farm that's already helpfully connected to the Internet.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIt is more dignified for humanity - a better look on our tombstone - if we die after the management of the AGI project was heroically warned of the dangers but came up with totally reasonable reasons to go ahead anyways.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nOr, failing that, if people made a heroic effort to do something that could maybe possibly have worked to generate a warning like that but couldn't actually in real life because the latest tensors were in a slightly different format and there was no time to readapt the methodology.  Compared to the much less dignified-looking situation if there's no warning and nobody even tried to figure out how to generate one.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nOr take MIRI.  Are we sad that it looks like this Earth is going to fail?  Yes.  Are we sad that we tried to do anything about that?  No, because it would be so much sadder, when it all ended, to face our ends wondering if maybe solving alignment would have just been as easy as buckling down and making a serious effort on it - not knowing if that would've just worked, if we'd only tried, because nobody had ever even tried at all.  It wasn't subjectively overdetermined that the (real) problems would be too hard for us, before we made the only attempt at solving them that would ever be made.  Somebody needed to try at all, in case that was all it took.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIt's sad that our Earth couldn't be one of the more dignified planets that makes a real effort, correctly pinpointing the actual real difficult problems and then allocating thousands of the sort of brilliant kids that our Earth steers into wasting their lives on theoretical physics.  But better MIRI's effort than nothing.  What were we supposed to do instead, pick easy irrelevant fake problems that we could make an illusion of progress on, and have nobody out of the human species even try to solve the hard scary real problems, until everybody just fell over dead?\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nThis way, at least, some people are walking around knowing why it is that if you train with an outer loss function that enforces the appearance of friendliness, you will not get an AI internally motivated to be friendly in a way that persists after its capabilities start to generalize far out of the training distribution...\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nTo be clear, nobody's going to listen to those people, in the end.  There will be more comforting voices that sound less politically incongruent with whatever agenda needs to be pushed forward that week.  Or even if that ends up not so, this isn't primarily a social-political problem, of just getting people to listen.  Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nBut compared to being part of a species that walks forward completely oblivious into the whirling propeller blades, with nobody having seen it at all or made any effort to stop it, it is dying with a little more dignity, if anyone knew at all.  You can feel a little incrementally prouder to have died as part of a species like that, if maybe not proud in absolute terms.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIf there is a stronger warning, because we did more transparency research?  If there's deeper understanding of the real dangers and those come closer to beating out comfortable nonrealities, such that DeepMind and Anthropic really actually back off from destroying the world and let Facebook AI Research do it instead?  If they try some hopeless alignment scheme whose subjective success probability looks, to the last sane people, more like 0.1% than 0?  Then we have died with even more dignity!  It may not get our survival probabilities much above 0%, but it would be so much more dignified than the present course looks to be!\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nNow of course the real subtext here, is that if you can otherwise set up the world so that it looks like you'll die with enough dignity - die of the social and technical problems that are really unavoidable, after making a huge effort at coordination and technical solutions and failing, rather than storming directly into the whirling helicopter blades as is the present unwritten plan -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- heck, if there was even a plan at all -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- then maybe possibly, if we're wrong about something fundamental, somehow, somewhere -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- in a way that makes things easier rather than harder, because obviously we're going to be wrong about all sorts of things, it's a whole new world inside of AGI -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- although, when you're fundamentally wrong about rocketry, this does not usually mean your rocket prototype goes exactly where you wanted on the first try while consuming half as much fuel as expected; it means the rocket explodes earlier yet, and not in a way you saw coming, being as wrong as you were -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- but if we get some miracle of unexpected hope, in those unpredicted inevitable places where our model is wrong -\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n- then our ability to take advantage of that one last hope, will greatly depend on how much dignity we were set to die with, before then.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIf we can get on course to die with enough dignity, maybe we won't die at all...?\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIn principle, yes.  Let's be very clear, though:  Realistically speaking, that is not how real life works.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nIt's possible for a model error to make your life easier.  But you do not get more surprises that make your life easy, than surprises that make your life even more difficult.  And people do not suddenly become more reasonable, and make vastly more careful and precise decisions, as soon as they're scared.  No, not even if it seems to you like their current awful decisions are weird and not-in-the-should-universe, and surely some sharp shock will cause them to snap out of that weird state into a normal state and start outputting the decisions you think they should make.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nSo don't get your heart set on that \"not die at all\" business.  Don't invest all your emotion in a reward you probably won't get.  Focus on dying with dignity - that is something you can actually obtain, even in this situation.  After all, if you help humanity die with even one more dignity point, you yourself die with one hundred dignity points!  Even if your species dies an incredibly undignified death, for you to have helped humanity go down with even slightly more of a real fight, is to die an extremely dignified death.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\n\"Wait, dignity points?\" you ask.  \"What are those?  In what units are they measured, exactly?\"\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nAnd to this I reply:  Obviously, the measuring units of dignity are over humanity's log odds of survival - the graph on which the logistic success curve is a straight line.  A project that doubles humanity's chance of survival from 0% to 0% is helping humanity die with one additional information-theoretic bit of dignity.\n=====\n!!!ai alignment\n!!!death with dignity\n!!!paragraph\n!!!lesswrong\n!!!eliezer yudkowsky\nBut if enough people can contribute enough bits of dignity like that, wouldn't that mean we didn't die at all?  Yes, but again, don't get your hopes up.  Don't focus your emotions on a goal you're probably not going to obtain.  Realistically, we find a handful of projects that contribute a few more bits of counterfactual dignity; get a bunch more not-specifically-expected bad news that makes the first-order object-level situation look even worse (where to second order, of course, the good Bayesians already knew that was how it would go); and then we all die.",
    "F5": "",
    "F6": "hi i am tab F6",
    "F7": "hi i am tab F7",
    "F8": "\n",
    "F9": "feature summary:\n\t- ChatGPT prompting\n\t- text-similarity search  (using OpenAI's embeddings endpoint)\n\t\t+ can update embeddings database from within app\n\t\t+ can filter by metadata before similarity search\n\t- can use placeholder values when prompting or searching embeddings\n\t\t+ example ChatGPT prompt: \n\t\t\t\"hey how do i do {f3} in python, in my program? here is my code: {main}\"\n\t- customizable layout\n\n\nhotkeys\n\n# global\nEscape -- bring all windows to the front\nctrl+s -- save text in current editor\nctrl+l -- load text from textfile into current editor\nF1-F9 -- open tab in scratchpads window (top left by default)\nctrl+q -- add the contents of the widget to the embeddings database\n\t- you can store multiple embeddings at once, by splitting them by a ===== line\n\t- lines starting with !!! will be used as metadata tags\n(alt+f4 closes everything, unlike the default tkinter behavior)\n\n# embedding window  (bottom left by default)\nctrl+e -- do similarity search\n\n# chatgpt window  (bottom right by default)\nctrl+g -- use api on current conversation\nctrl+1 -- add \"system\" message\nctrl+2 -- add \"user\" message\nctrl+3 -- add \"assistant\" message\nctrl+r -- remove last message\n\n# placeholder values\nif you write {placeholder} anywhere, it will replace it with the text that is referred, for any API calls\nis case-insensitive\n## available values:\n\t- f1 to f9: scratchpad tab contents.\n\t\texample: {f1}\n\t- main: main editor contents  (top left)\n\t- chatgpt.full: chatgpt conversation\n\t- chatgpt.latest: last message in chatgpt conversation\n\t- chatgpt.last: same as above.\n\n\n\n# todo:\n\t- helpers for adding to embeddings database, like, chopping text into paragraphs, picking metadata for multiple chunks of text"
}